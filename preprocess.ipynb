{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"tweets/tweets/5g-corona/5g_corona_conspiracy.json\"\n",
    "corona_data = json.load(open(path))\n",
    "path = \"tweets/tweets/non-conspiracy/non_conspiracy.json\"\n",
    "non_data = json.load(open(path))\n",
    "path = \"tweets/tweets/Other-Conspiracy/other_conspiracy.json\"\n",
    "other_data = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_texts = np.array([corona_data[i]['full_text'] for i in range(len(corona_data))])\n",
    "non_texts = np.array([non_data[i]['full_text'] for i in range(len(non_data))])\n",
    "other_texts = np.array([other_data[i]['full_text'] for i in range(len(other_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4957142857142857\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for text in other_texts:\n",
    "    if 'http' in text:\n",
    "        count += 1\n",
    "print(count/len(other_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_df = pd.DataFrame(corona_texts, columns= [ 'body' ])\n",
    "non_df = pd.DataFrame(non_texts, columns= [ 'body' ])\n",
    "other_df = pd.DataFrame(other_texts, columns= [ 'body' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is mad 5g poles just popping up everywher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Former Vodafone Boss Blows Whistle on 5G Coron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay so if the 5G theory isn’t true or have so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@DB1_2 @JayMcCluskey74 @NotArsedLike2 I can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Conspiracy theories online suggest #5G #techno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body\n",
       "0  This is mad 5g poles just popping up everywher...\n",
       "1  Former Vodafone Boss Blows Whistle on 5G Coron...\n",
       "2  Okay so if the 5G theory isn’t true or have so...\n",
       "3  @DB1_2 @JayMcCluskey74 @NotArsedLike2 I can't ...\n",
       "4  Conspiracy theories online suggest #5G #techno..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"body\"] = df_copy[\"body\"].str.lower()\n",
    "    df_copy[\"body\"] = df_copy.apply(lambda row: word_tokenize(row['body']), axis = 1)\n",
    "    df_copy[\"body\"] = df_copy.apply(lambda row: [word for word in row[\"body\"] if (word.isalpha() or any(map(str.isdigit, word)))], axis = 1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df):\n",
    "    df_copy = df.copy()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_copy[\"body\"] = df_copy.apply(lambda row: [lemmatizer.lemmatize(word) for word in row[\"body\"]], axis = 1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/?fbclid=IwAR2qZjnIsSyY_3OGBPg_lovC9o0uKWM8dGq8Ne22bMVHxG8E7OE8SPAyNxE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_pos(df):\n",
    "    df_copy = df.copy()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df_copy[\"content\"] = df_copy.apply(lambda row: [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in row[\"content\"]], axis = 1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vector(df):\n",
    "    df_copy = df.copy()\n",
    "    tfidf = TfidfVectorizer(analyzer='word', tokenizer=lambda _: _, preprocessor=lambda _: _, token_pattern=None, stop_words='english')\n",
    "    tvec_weights = tfidf.fit_transform(df_copy[\"body\"])\n",
    "    # print(tvec_weights.toarray())\n",
    "    weights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'term': tfidf.get_feature_names(), 'weight': weights})\n",
    "    weights_sorted = weights_df.sort_values(by = 'weight', ascending = False)\n",
    "    print(weights_sorted.head(10))\n",
    "    return tvec_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_content = corona_df.pipe(preprocess).pipe(lemmatize)\n",
    "non_content = non_df.pipe(preprocess).pipe(lemmatize)\n",
    "other_content = other_df.pipe(preprocess).pipe(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for tweets_token in corona_content['body'].to_numpy():\n",
    "    for word in tweets_token:\n",
    "        if word not in stop_words:\n",
    "            fdist[word.lower()]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_buzz_words = ['coronavirus', 'virus', 'wuhan', 'corona', 'pandemic', 'covid', 'symptom', 'vaccine', 'flu', 'covid19', 'covid-19']\n",
    "g5_buzz_words = ['network', 'radiation', '5g', '4g', 'tower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06398274622573688\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "thres = 2\n",
    "data = non_content['body'].to_numpy()\n",
    "for x in data:\n",
    "    g5_count = 0\n",
    "    corona_count = 0\n",
    "    for word in x:\n",
    "        if word in corona_buzz_words:\n",
    "            corona_count += 1\n",
    "        if word in g5_buzz_words:\n",
    "            g5_count += 1\n",
    "        if corona_count >= thres and g5_count >= thres:\n",
    "            count += 1\n",
    "            break\n",
    "print(count/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.79823008849557"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(x) for x in df_content['body'].to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             term    weight\n",
      "2022         http  0.042232\n",
      "892   coronavirus  0.037450\n",
      "4526        virus  0.026956\n",
      "3099       people  0.024960\n",
      "4549           wa  0.022282\n",
      "4704        wuhan  0.021317\n",
      "886        corona  0.021009\n",
      "4288        tower  0.019724\n",
      "3632            s  0.019110\n",
      "169           amp  0.018913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\11956\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "vectors = df_content.pipe(tfidf_vector).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4758"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09590129288870966"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyclustertend as pct\n",
    "pct.hopkins(vectors, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "summary = []\n",
    "for i in range(50, 100):\n",
    "    labels = KMeans(n_clusters=i, random_state=None).fit_predict(vectors)\n",
    "    result = metrics.silhouette_score(vectors, labels)\n",
    "    summary.append(result)\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "l = np.asarray(summary)\n",
    "print(l.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.035689733224199875,\n",
       " 0.03016618150274908,\n",
       " 0.03231839073518291,\n",
       " 0.0342456455092172,\n",
       " 0.01085831572895175,\n",
       " 0.033461297892968915,\n",
       " -0.0010061637525390532,\n",
       " 0.026105963608349783,\n",
       " 0.023765732232693796,\n",
       " -0.014092954526579191,\n",
       " -0.010043854324797315,\n",
       " 0.03716178956304722,\n",
       " 0.022845862199821085,\n",
       " 0.03176113781413042,\n",
       " 0.027968472033004333,\n",
       " 0.01983264412247695,\n",
       " 0.035113104771028344,\n",
       " 0.036027582821125975,\n",
       " 0.033409800108821004,\n",
       " -0.016312964235137055,\n",
       " -0.008644114852455654,\n",
       " -0.01497617070300697,\n",
       " 0.025027930951929213,\n",
       " -0.012033584717626732,\n",
       " -0.013634612663815814,\n",
       " -0.015795155536321343,\n",
       " -0.012964892707546414,\n",
       " -0.011767787735382437,\n",
       " -0.0056388472959391905,\n",
       " -0.017386388310902147,\n",
       " -0.008033269639290049,\n",
       " -0.021976473341743418,\n",
       " -0.013607819864435494,\n",
       " -0.01635261525803282,\n",
       " -0.015432414623469443,\n",
       " -0.0103653122350058,\n",
       " 0.036551525132938835,\n",
       " 0.029752526331921103,\n",
       " 0.02768280735452487,\n",
       " 0.007764548033019912,\n",
       " -0.014112950025018054,\n",
       " -0.00827948204194722,\n",
       " -0.009893352698343457,\n",
       " -0.01765568715082467,\n",
       " -0.009349994072755932,\n",
       " -0.007033814707495146,\n",
       " -0.012935204876206356,\n",
       " -0.012608629801224604,\n",
       " -0.011198968281712112,\n",
       " -0.017071812292250837]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
