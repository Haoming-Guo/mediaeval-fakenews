{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bilstm_text",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_VACd3mFd2h"
      },
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxxKVwEw2KEh"
      },
      "source": [
        "# Upload needed files (if any)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpP16dp0zxnu"
      },
      "source": [
        "# Installing needed packages.\n",
        "!pip install transformers\n",
        "!pip install flair\n",
        "#!pip install nlpaug\n",
        "#!pip install googletrans\n",
        "!pip install twokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuG7lmEK2-1K"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Model,Sequential, load_model\n",
        "from keras.layers import (Bidirectional, Concatenate, Conv1D, Dense,BatchNormalization,\n",
        "                          Dropout, Embedding, GlobalMaxPooling1D, Input,\n",
        "                          LSTM, TimeDistributed, Activation, Flatten, Lambda)\n",
        "from keras.callbacks import EarlyStopping\n",
        "import keras\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import nlpaug.augmenter.word as naw\n",
        "from nltk.tokenize import word_tokenize\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "import twokenize\n",
        "from gensim.models.wrappers import FastText\n",
        "import gensim\n",
        "#from googletrans import Translator\n",
        "import tensorflow as tf\n",
        "import flair\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "np.random.seed(1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQB-YoR3qEg"
      },
      "source": [
        "# define paths\n",
        "root = '/content/drive/My Drive/fake-news/'\n",
        "word2vec_path = root + 'wiki-news-300d-1M.vec'\n",
        "new_word2vec_path = root + 'GoogleNews-vectors-negative300.bin'\n",
        "corona_path = root + \"5g_corona_conspiracy.json\"\n",
        "non_path = root + \"non_conspiracy.json\"\n",
        "other_path = root + \"other_conspiracy.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n83l7RdmKyVQ"
      },
      "source": [
        "embedding_dict = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA6Y9YqU4sru"
      },
      "source": [
        "# load word2vec\n",
        "def load_vectors(fname, thres=100000):\n",
        "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    i = 0\n",
        "    for line in tqdm(fin):\n",
        "        i += 1\n",
        "        if i > thres:\n",
        "            break\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
        "    return data\n",
        "word2vec = load_vectors(word2vec_path, thres=140000)\n",
        "d = dict(word2vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8t0xoKe4L6U"
      },
      "source": [
        "# load data\n",
        "import json\n",
        "import numpy as np\n",
        "corona_data = json.load(open(corona_path))\n",
        "non_data = json.load(open(non_path))\n",
        "other_data = json.load(open(other_path))\n",
        "corona_texts = np.array([corona_data[i]['full_text'] for i in range(len(corona_data))])\n",
        "non_texts = np.array([non_data[i]['full_text'] for i in range(len(non_data))])\n",
        "other_texts = np.array([other_data[i]['full_text'] for i in range(len(other_data))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn3dYD1OTqAF"
      },
      "source": [
        "corona_texts[30:50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jObdMIT8WwY"
      },
      "source": [
        "#Augment by inserting similar words\n",
        "def aug_insert(texts, n=1):\n",
        "    aug_data = texts.copy()\n",
        "    # model could be roberta, bert and distilbert; aug_p is percentage of words to be augmented\n",
        "    aug = naw.ContextualWordEmbsAug(model_path='roberta-base', action=\"insert\")\n",
        "    for text in tqdm(texts):\n",
        "        augmented_texts = aug.augment(text, n=n)\n",
        "        aug_data.extend(augmented_texts)\n",
        "    return aug_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLA0Nj219w9M"
      },
      "source": [
        "# Augment by replacing similar words\n",
        "def aug_sub(texts, n=1):\n",
        "    aug_data = texts.copy()\n",
        "    # model could be roberta, bert and distilbert; aug_p is percentage of words to be augmented\n",
        "    aug = naw.ContextualWordEmbsAug(model_path='roberta-base', action=\"substitute\")\n",
        "    for text in tqdm(texts):\n",
        "        augmented_texts = aug.augment(text, n=n)\n",
        "        aug_data.extend(augmented_texts)\n",
        "    return aug_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYi9aUVt-4Iv"
      },
      "source": [
        "# Augment by randomly deleting sentecnes\n",
        "def aug_rm(texts, n=1):\n",
        "    aug_data = texts.copy()\n",
        "    # aug_p is percentage of words to be augmented\n",
        "    aug = naw.RandomWordAug(aug_p=0.3)\n",
        "    for text in tqdm(texts):\n",
        "        augmented_texts = aug.augment(text, n=n)\n",
        "        aug_data.extend(augmented_texts)\n",
        "    return aug_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AeDIemm95rD"
      },
      "source": [
        "# augment by translation and back translation\n",
        "def aug_translate(texts, n=1):\n",
        "    aug_data = texts.copy()\n",
        "    aug = naw.BackTranslationAug(\n",
        "    from_model_name='transformer.wmt19.en-de', \n",
        "    to_model_name='transformer.wmt19.de-en'\n",
        "    )\n",
        "    for text in tqdm(texts):\n",
        "        augmented_texts = aug.augment(text, n=n)\n",
        "        aug_data.extend(augmented_texts)\n",
        "    return aug_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps19ik7PL-V6"
      },
      "source": [
        "def aug_translate(texts, n=1):\n",
        "    aug_data = texts.copy()\n",
        "    lang_list = ['zh-cn', 'de', 'es', 'vi', 'fr']\n",
        "    translator = Translator()\n",
        "    for text in tqdm(texts):\n",
        "        for lang in lang_list[:n]:\n",
        "            temp = translator.translate(text, dest=lang)\n",
        "            augmented_text = translator.translate(temp.text, src=lang, dest='en')\n",
        "            aug_data.append(augmented_text.text)\n",
        "    return aug_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQtnW3qOEXkL"
      },
      "source": [
        "# convert to 2 class\n",
        "other_texts = np.concatenate([non_texts, other_texts])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbV_f4AnAUQL"
      },
      "source": [
        "# split train/val and execute Augmentation\n",
        "train_corona_texts, val_corona_texts = train_test_split(corona_texts, test_size=200, random_state=0)\n",
        "train_other_texts, val_other_texts = train_test_split(other_texts, test_size=200, random_state=0)\n",
        "#aug_n = len(train_other_texts) // len(train_corona_texts) - 1\n",
        "#train_corona_texts = aug_translate(train_corona_texts.tolist(), n=4)\n",
        "#train_other_texts = train_other_texts[-900:]\n",
        "#train_corona_texts = np.concatenate([train_corona_texts,train_corona_texts,train_corona_texts,train_corona_texts])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVn9cXgvXEIT"
      },
      "source": [
        "# Save aug data\n",
        "with open(root+'train_corona_aug_translate.json', 'w') as f:\n",
        "    json.dump(train_corona_texts, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVOcCMlSMia7"
      },
      "source": [
        "# Load aug data\n",
        "with open(root+'train_corona_aug_translate.json') as f:\n",
        "    train_corona_texts = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuUkuBhoA-PB"
      },
      "source": [
        "count = 0\n",
        "val_corona_texts = []\n",
        "for t1 in corona_texts:\n",
        "  add = True\n",
        "  for t2 in train_corona_texts:\n",
        "    if t1==t2:\n",
        "      add = False\n",
        "      break\n",
        "  if add:\n",
        "    val_corona_texts.append(t1)\n",
        "val_corona_texts = np.array(val_corona_texts)\n",
        "len(val_corona_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sswi0kCBjwx"
      },
      "source": [
        "def preprocess(sen, tokenize):\n",
        "    tokens = tokenize(sen)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9zXZ2Dm_EyX"
      },
      "source": [
        "# Tokenize\n",
        "tokenize = twokenize.tokenizeRawTweetText #word_tokenize \n",
        "train_tokens = [tokenize(sen) for sen in np.concatenate([train_corona_texts,train_other_texts])]\n",
        "train_labels = np.concatenate([[0]*len(train_corona_texts),[1]*len(train_other_texts)])\n",
        "val_tokens = [tokenize(sen) for sen in np.concatenate([val_corona_texts,val_other_texts])]\n",
        "val_labels = np.concatenate([[0]*len(val_corona_texts),[1]*len(val_other_texts)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uqLF6qXkCWr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4sdFGnQM_l1"
      },
      "source": [
        "# 3 class\n",
        "train_corona_texts, val_corona_texts = train_test_split(corona_texts, test_size=100, random_state=0)\n",
        "#train_corona_texts = aug_translate(train_corona_texts.tolist(), n=2)\n",
        "#train_corona_texts = json.load(open(root+'train_corona_translate_s0_n2.json'))\n",
        "train_non_texts, val_non_texts = train_test_split(non_texts, test_size=100, random_state=0)\n",
        "train_non_texts = train_non_texts[:3000]\n",
        "train_other_texts, val_other_texts = train_test_split(other_texts, test_size=100, random_state=0)\n",
        "#train_other_texts = aug_translate(train_other_texts.tolist(), n=4)\n",
        "train_other_texts = json.load(open(root+'train_other_translate_s0_n4.json'))\n",
        "train_tokens = [word_tokenize(sen) for sen in np.concatenate([train_corona_texts,train_non_texts,train_other_texts])]\n",
        "val_tokens = [word_tokenize(sen) for sen in np.concatenate([val_corona_texts,val_non_texts,val_other_texts])]\n",
        "#train_tokens = [twokenize.tokenizeRawTweetText(sen) for sen in np.concatenate([train_corona_texts,train_non_texts,train_other_texts])]\n",
        "#val_tokens = [twokenize.tokenizeRawTweetText(sen) for sen in np.concatenate([val_corona_texts,val_non_texts,val_other_texts])]\n",
        "#train_labels = np.concatenate([[0]*len(train_corona_texts),[1]*len(train_non_texts),[2]*len(train_other_texts)])\n",
        "#val_labels = np.concatenate([[0]*len(val_corona_texts),[1]*len(val_non_texts),[2]*len(val_other_texts)])\n",
        "\n",
        "# with open(root+'train_corona_translate_s0_n2.json', 'w') as f:\n",
        "#     json.dump(train_corona_texts, f)\n",
        "# with open(root+'train_other_translate_s0_n4.json', 'w') as f:\n",
        "#     json.dump(train_other_texts, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT3eRBwWd3Fn"
      },
      "source": [
        "all_texts = np.concatenate([corona_texts, other_texts, non_texts])\n",
        "all_labels = np.concatenate([[0]*len(corona_texts), [1]*len(other_texts), [2]*len(non_texts)])\n",
        "train_texts, test_texts, train_ys, test_ys = train_test_split(all_texts, all_labels, test_size=0.2, random_state=0)\n",
        "tokenize = twokenize.tokenizeRawTweetText\n",
        "train_tokens = [preprocess(sen, tokenize) for sen in train_texts]\n",
        "val_tokens = [preprocess(sen, tokenize) for sen in test_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28NIJhLM6YBZ"
      },
      "source": [
        "# Clean dictionary and save space\n",
        "news_dict = {}\n",
        "n_tokens = 0\n",
        "for c in [train_tokens, val_tokens]:\n",
        "    for s in c:\n",
        "        for w in s:\n",
        "            w = w.lower()\n",
        "            if w in d.keys():\n",
        "                news_dict[w] = d[w]\n",
        "            elif w == 'wuhan':\n",
        "                news_dict[w] = d['Wuhan']\n",
        "            elif w == 'covid' or w == 'covid19' or w == 'covid-19' or w == 'covid_19':\n",
        "                news_dict[w] = d['coronavirus']\n",
        "            #else:\n",
        "                #print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GokQfDaq7JdY"
      },
      "source": [
        "# root = '/content/drive/My Drive/fake-news/'\n",
        "# with open(root+'tokens.json') as f:\n",
        "#     tokens = json.load(f)\n",
        "# with open(root+'y_data.json') as f:\n",
        "#     y = json.load(f)\n",
        "# with open(root+'news_dict.json') as f:\n",
        "#     news_dict = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCJrlMET4q5y"
      },
      "source": [
        "test_texts[inds[0][2]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thbA6lN-4TIq"
      },
      "source": [
        "np.array(val_tokens)[inds][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWGsGEWOIXCL"
      },
      "source": [
        "# prepare for training\n",
        "#y = np_utils.to_categorical(np.array(y))\n",
        "train_labels = train_ys\n",
        "val_labels = test_ys\n",
        "train_y = np_utils.to_categorical(train_labels)\n",
        "val_y = np_utils.to_categorical(val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZxJpgI1HbV8"
      },
      "source": [
        "n_symbols = len(news_dict.items()) + 1  # amount of words\n",
        "embedding_weights = np.zeros((n_symbols, 300))  \n",
        "index = 0\n",
        "index_dict = {}\n",
        "for x in news_dict.items():\n",
        "    index += 1\n",
        "    word = x[0]\n",
        "    index_dict[word] = index\n",
        "    embedding_weights[index, :] = news_dict[word]  # word vectors' metrix, embedding_weight[0]=0 (index begins from 0)\n",
        "\n",
        "lens = [len([0 for w in x if (w in index_dict.keys() and w.isalpha())]) for x in np.concatenate([train_tokens, val_tokens])]\n",
        "maxlen = np.amax(lens)\n",
        "print(np.concatenate([train_tokens, val_tokens])[np.argmax(lens)])\n",
        "train_x = []\n",
        "for i, words in enumerate(train_tokens):    \n",
        "    sen = []\n",
        "    for i, w in enumerate(words):\n",
        "        if not w.isalpha():\n",
        "            continue # Skip punctuations\n",
        "        try:\n",
        "            sen.append(index_dict[w])\n",
        "        except:\n",
        "            continue   \n",
        "    train_x.append(sen)\n",
        "train_x = sequence.pad_sequences(np.array(train_x), maxlen = maxlen)\n",
        "\n",
        "val_x = []\n",
        "for i, words in enumerate(val_tokens):    \n",
        "    sen = []\n",
        "    for i, w in enumerate(words):\n",
        "        if not w.isalpha():\n",
        "            continue # Skip punctuations\n",
        "        try:\n",
        "            sen.append(index_dict[w])\n",
        "        except:\n",
        "            continue\n",
        "    val_x.append(sen)\n",
        "val_x = sequence.pad_sequences(np.array(val_x), maxlen = maxlen)\n",
        "\n",
        "print(train_x.shape, val_x.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U995Xb1VPxpB"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "train_x, train_y = shuffle(train_x, train_y, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHBcd2kJlVRQ"
      },
      "source": [
        "# def split(x, y):\n",
        "#     train_x = np.concatenate([x[:1000], x[-1300:-100]])\n",
        "#     train_y = np.concatenate([y[:1000], y[-1300:-100]])\n",
        "#     test_x = np.concatenate([x[1000:1150], x[2300:2400], x[-100:]])\n",
        "#     test_y = np.concatenate([y[1000:1150], y[2300:2400], y[-100:]])\n",
        "#     return train_x, test_x, train_y,train_y\n",
        "# train_x, test_x, train_y, test_y = split(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NA4RQPj3IUt"
      },
      "source": [
        "\n",
        "class BaseBiLSTM(object):\n",
        "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
        "                 embedding_weights, embedding_size=100):\n",
        "        self.model = None\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        self.embedding_weights = embedding_weights\n",
        "        self.labels = labels\n",
        "        self.n_labels = 3\n",
        "    \n",
        "    def add_input_layer(self):\n",
        "        return Input(shape=(self.max_sentence_length, ))\n",
        "        \n",
        "    def add_embedding_layer(self, layers):\n",
        "        layers = Embedding(\n",
        "            input_dim=self.vocabulary_size,\n",
        "            output_dim=self.embedding_size,\n",
        "            weights = [self.embedding_weights],\n",
        "            input_length = self.max_sentence_length)(layers)\n",
        "        return layers\n",
        "    \n",
        "    def add_recurrent_layer(self, layers):\n",
        "        layers = Bidirectional(\n",
        "            LSTM(units=256, return_sequences=True,\n",
        "                 recurrent_dropout=0.3))(layers)\n",
        "        return layers\n",
        "    \n",
        "    def add_output_layer(self, layers):\n",
        "        layers = Dense(self.n_labels, activation='softmax')(layers)\n",
        "        return layers\n",
        "    \n",
        "    def build(self):\n",
        "        inputs = self.add_input_layer()\n",
        "        layers = self.add_embedding_layer(inputs)\n",
        "        layers = Dropout(0.3)(layers)\n",
        "        layers = self.add_recurrent_layer(layers)\n",
        "        layers = Dropout(0.3)(layers)\n",
        "        layers = Dense(64, activation='relu')(layers)\n",
        "        layers = Flatten()(layers)\n",
        "        layers = Dense(64, activation='relu')(layers)\n",
        "        layers = Dropout(0.3)(layers)\n",
        "        layers = Dense(32, activation='relu')(layers)\n",
        "        outputs = self.add_output_layer(layers)        \n",
        "        \n",
        "        self.model = Model(inputs=inputs, outputs=outputs)\n",
        "        opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "    \n",
        "    def fit(self, X_train, y_train, epochs, batch_size=128, validation_split=0.2):\n",
        "        if self.model is None:\n",
        "            self.build()\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=5,verbose = 2)\n",
        "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "                              validation_split=validation_split, callbacks=[early_stopping], shuffle=True, class_weight={0: 0.4, 1: 0.5, 2:0.1})\n",
        "    \n",
        "    def predict(self, X_test):\n",
        "        return np.argmax(self.model.predict(X_test), axis=-1)\n",
        "    \n",
        "    def evaluate(self, X_test, y_test, cm=False):\n",
        "        predictions = np.argmax(self.model.predict(X_test), axis=-1).flatten()\n",
        "        true_labels = np.argmax(y_test, axis=-1).flatten()\n",
        "        print(classification_report(true_labels, predictions))\n",
        "        if cm:\n",
        "            seaborn.heatmap(\n",
        "                metrics.confusion_matrix(true_labels, predictions, labels=range(6)))\n",
        "\n",
        "\n",
        "senti_label = np.array([0,1,2])\n",
        "model = BaseBiLSTM(\n",
        "    vocabulary_size=len(index_dict) + 1, max_sentence_length=maxlen, \n",
        "    embedding_weights = embedding_weights, labels=senti_label, embedding_size=300)\n",
        "# model = BaseBiLSTM(\n",
        "#     vocabulary_size=0, max_sentence_length=maxlen, \n",
        "#     embedding_weights = 0, labels=senti_label, embedding_size=0)\n",
        "model.build()\n",
        "model.model.summary()\n",
        "\n",
        "\n",
        "model.fit(X_train=train_x, y_train=train_y, epochs=1)\n",
        "\n",
        "#model.evaluate(test_x, test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaC5AyJPQmHJ"
      },
      "source": [
        "#for i in range(2):\n",
        "model.fit(X_train=train_x, y_train=train_y, epochs=1)\n",
        "print(matthews_corrcoef(model.predict(val_x), val_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOkO7hvOkqaL"
      },
      "source": [
        "model.evaluate(val_x, val_y)\n",
        "import sklearn\n",
        "sklearn.metrics.roc_auc_score(y_true=val_y, y_score=model.model.predict(val_x), multi_class='ovo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miiREKKz9l9U"
      },
      "source": [
        "matthews_corrcoef(val_labels, model.predict(val_x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX7zFw553oPe"
      },
      "source": [
        "model.model.save(root+'best_bilstm_03')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgaC48D30tdx"
      },
      "source": [
        "roberta_probs = pd.read_csv(root+'roberta_prob.csv')\n",
        "roberta_probs.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icsiTQGTgqX7"
      },
      "source": [
        "train_probs = np.concatenate([clf.predict_proba(train_bert), test_model.predict(train_x)], axis=-1) #(#data, 2+2+1)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#ensemble = DecisionTreeClassifier(random_state=0).fit(train_probs, train_labels)\n",
        "ensemble = MLPClassifier(hidden_layer_sizes=(5,), alpha=0.0005, random_state=0).fit(train_probs, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi6yHe47gP6g"
      },
      "source": [
        "val_probs = np.concatenate([clf.predict_proba(test_bert), test_model.predict(val_x)], axis=-1)\n",
        "ensemble.score(val_probs, val_labels), matthews_corrcoef(ensemble.predict(val_probs), val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxnWgM68gqVG"
      },
      "source": [
        "matthews_corrcoef(np.argmax(test_model.predict(val_x), axis=-1), val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVE1xdkTkvOs"
      },
      "source": [
        "preds = model.predict(val_x)\n",
        "inds_lstm = np.where(model.predict(val_x) - val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OhMEG4PU2Yo"
      },
      "source": [
        "for i in inds_lstm[0][:10]:\n",
        "  print(val_labels[i], test_texts[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSy5McWEJz91"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "inp = test_model.input                                           # input placeholder\n",
        "outputs = [layer.output for layer in test_model.layers]          # all layer outputs\n",
        "functors = [K.function([inp], [out]) for out in outputs]    # evaluation functions\n",
        "\n",
        "# Testing\n",
        "test = np.array([val_x[0]])\n",
        "layer_outs = [func([test]) for func in functors]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OndErv0bK00N"
      },
      "source": [
        "train_lstm_emb = []\n",
        "for s in train_x:\n",
        "  test = np.array([s])\n",
        "  layer_outs = [func([test]) for func in functors]\n",
        "  emb = layer_outs[-2][0][0]\n",
        "  train_lstm_emb.append(emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOkn31yhMbpW"
      },
      "source": [
        "layer_outs = [func([train_x]) for func in functors]\n",
        "train_lstm_emb = layer_outs[-2][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xggvs9RK0nz"
      },
      "source": [
        "layer_outs = [func([val_x]) for func in functors]\n",
        "val_lstm_emb = layer_outs[-2][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW2jgK6VOxhC"
      },
      "source": [
        "print(train_lstm_emb.shape, len(train_labels))\n",
        "train_both_emb = np.concatenate([train_lstm_emb, train_bert], axis=-1)\n",
        "val_both_emb = np.concatenate([val_lstm_emb, test_bert], axis=-1)\n",
        "temp =MLPClassifier(hidden_layer_sizes=(300,), alpha=0.0005, random_state=0, max_iter=100).fit(train_both_emb, train_labels)\n",
        "temp.score(val_both_emb, val_labels), matthews_corrcoef(temp.predict(val_both_emb), val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIlgQftmjZz5"
      },
      "source": [
        "train_probs = np.concatenate([clf.predict_proba(train_bert), model.model.predict(train_x)], axis=-1) #(#data, 2+2+1)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#ensemble = DecisionTreeClassifier(random_state=0).fit(train_probs, train_labels)\n",
        "ensemble = MLPClassifier(hidden_layer_sizes=(5,), alpha=0.0001, random_state=0).fit(train_probs, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra36aWl2OgSq"
      },
      "source": [
        "val_probs = np.concatenate([clf.predict_proba(test_bert), model.model.predict(val_x)], axis=-1)\n",
        "ensemble.score(val_probs, val_labels), matthews_corrcoef(ensemble.predict(val_probs), val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTo3rJBOOgdA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3JGy_Z6sG7o"
      },
      "source": [
        "model = BaseBiLSTM(\n",
        "    vocabulary_size=len(index_dict) + 1, max_sentence_length=maxlen, \n",
        "    embedding_weights = embedding_weights, labels=senti_label, embedding_size=300)\n",
        "# model = BaseBiLSTM(\n",
        "#     vocabulary_size=0, max_sentence_length=maxlen, \n",
        "#     embedding_weights = 0, labels=senti_label, embedding_size=0)\n",
        "model.build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6LCjZId6PdY"
      },
      "source": [
        "from tensorflow import keras\n",
        "test_model = keras.models.load_model(root+'best_bilstm_03')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujxeMfTD6PaV"
      },
      "source": [
        "good_model = model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHQ5ar72ZjHf"
      },
      "source": [
        "import pandas as pd\n",
        "c_csv = pd.read_csv(root + \"corona_data.csv\")\n",
        "c_csv = c_csv.drop(['user/location', 'user/created_at_month', 'user/profile_background_tile', 'user/profile_use_background_image',\t'user/has_extended_profile',\t'user/default_profile',\t'user/default_profile_image'], axis=1)\n",
        "o_csv = pd.read_csv(root + \"other_conspiracy_data.csv\")\n",
        "o_csv = o_csv.drop(['user/location', 'user/created_at_month', 'user/profile_background_tile', 'user/profile_use_background_image',\t'user/has_extended_profile',\t'user/default_profile',\t'user/default_profile_image'], axis=1)\n",
        "n_csv = pd.read_csv(root + \"non_conspiracy_data.csv\")\n",
        "n_csv = n_csv.drop(['user/location', 'user/created_at_month', 'user/profile_background_tile', 'user/profile_use_background_image',\t'user/has_extended_profile',\t'user/default_profile',\t'user/default_profile_image'], axis=1)\n",
        "c_csv['class'] = 0\n",
        "n_csv['class'] = 1\n",
        "# o_csv_three = o_csv.copy()\n",
        "# o_csv_three['class'] = 2\n",
        "o_csv['class'] = 1\n",
        "all_csv = c_csv.append(n_csv).append(o_csv)\n",
        "#all_three_csv = c_csv.append(n_csv).append(o_csv_three)\n",
        "#cn_csv = c_csv.append(n_csv)\n",
        "ys = np.concatenate([np.zeros(len(corona_texts)), np.ones(len(other_texts)-700), np.ones(700)*2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGe24zJnZ2YY"
      },
      "source": [
        "data = all_csv.to_numpy()[:,:-1]\n",
        "train_csv, test_csv, train_csv_y, test_csv_y = train_test_split(data, ys, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuiye9qVdv0t"
      },
      "source": [
        "count = [0,0,0]\n",
        "new_x, new_y = [], []\n",
        "threshold = 600\n",
        "for i, x in enumerate(train_csv):\n",
        "  if count[int(train_csv_y[i])] < threshold:\n",
        "    count[int(train_csv_y[i])] += 1\n",
        "    new_x.append(x)\n",
        "    new_y.append(train_csv_y[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5sOsL9samx0"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler().fit(new_x)\n",
        "new_x = scaler.transform(new_x)\n",
        "test_csv = scaler.transform(test_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r5svJDlbGtg"
      },
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(30,30,30), learning_rate_init=0.001, random_state=0, max_iter=500).fit(new_x, new_y)\n",
        "clf.score(test_csv, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOFqAjnCbVEw"
      },
      "source": [
        "matthews_corrcoef(clf.predict(test_csv), test_csv_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwgS6_ecbles"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=40, max_depth=5, random_state=0)\n",
        "clf.fit(new_x, new_y)\n",
        "clf.score(test_csv, val_labels), matthews_corrcoef(clf.predict(test_csv), val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-NBx7nfZ2S7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgzYPDvSZjUf"
      },
      "source": [
        "def get_hashtag(tokens):\n",
        "    tags = []\n",
        "    for w in tokens:\n",
        "      if w[0] == '#':\n",
        "        tags.append(w)\n",
        "    return tags\n",
        "tags_data = [get_hashtag(sen) for sen in train_tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbxlUoQfZjPp"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(analyzer='word', tokenizer=lambda _: _, preprocessor=lambda _: _, token_pattern=None, ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
        "X = np.asarray(tfidf.fit_transform(tags_data).todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX3GtVlPdi5o"
      },
      "source": [
        "train_tags, val_tags, train_tags_y, val_tags_y = train_test_split(X, train_labels, test_size=0.2, random_state=0)\n",
        "tags_clf = MLPClassifier(hidden_layer_sizes=(10,), learning_rate_init=0.001, random_state=0, max_iter=50).fit(train_tags, train_tags_y)\n",
        "tags_clf.score(val_tags, val_tags_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ9MCCZfiJvW"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "tags_clf = RandomForestClassifier(n_estimators=20, max_depth=3, random_state=0, class_weight={0:0.8,1:0.2})\n",
        "tags_clf.fit(train_tags, train_tags_y, )\n",
        "tags_clf.score(val_tags, val_tags_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN7IoL9xepOS"
      },
      "source": [
        "matthews_corrcoef(tags_clf.predict(val_tags), val_tags_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hJdPYZqZjEd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNlLnCNrZjBB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1esWafKqsG0w"
      },
      "source": [
        "train_corona_tokens = [train_x[i] for i in range(len(train_tokens)) if train_labels[i] == 0]\n",
        "train_other_tokens = [train_x[i] for i in range(len(train_tokens)) if train_labels[i] == 1]\n",
        "val_corona_tokens = [val_x[i] for i in range(len(val_tokens)) if val_labels[i] == 0]\n",
        "val_other_tokens = [val_x[i] for i in range(len(val_tokens)) if val_labels[i] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB-R-u7GsGw8"
      },
      "source": [
        "np.random.seed(0)\n",
        "negatives_train = []\n",
        "train_labels_pair = []\n",
        "ref_size = 30\n",
        "ref_tokens = train_other_tokens[-ref_size:]\n",
        "train_other_tokens = train_other_tokens[:-ref_size]\n",
        "for c_t in train_corona_tokens:\n",
        "    for _ in range(10):\n",
        "        i = np.random.randint(0, len(ref_tokens))\n",
        "        o_t = ref_tokens[i]\n",
        "        sample = np.concatenate([[c_t], [o_t]])\n",
        "        negatives_train.append(sample)\n",
        "        train_labels_pair.append(0)\n",
        "positives_train = []\n",
        "for c_t in train_other_tokens:\n",
        "    for _ in range(2):\n",
        "        i = np.random.randint(0, len(ref_tokens))\n",
        "        o_t = ref_tokens[i]\n",
        "        sample = np.concatenate([[c_t], [o_t]])\n",
        "        positives_train.append(sample)\n",
        "        train_labels_pair.append(1)\n",
        "train_x_pair = np.concatenate([negatives_train, positives_train])\n",
        "train_y_pair = np_utils.to_categorical(train_labels_pair)\n",
        "\n",
        "negatives_val = []\n",
        "val_labels_pair = []\n",
        "for c_t in val_corona_tokens:\n",
        "    for _ in range(5):\n",
        "        i = np.random.randint(0, len(ref_tokens))\n",
        "        o_t = ref_tokens[i]\n",
        "        sample = np.concatenate([[c_t], [o_t]])\n",
        "        negatives_val.append(sample)\n",
        "        val_labels_pair.append(0)\n",
        "positives_val = []\n",
        "for c_t in val_other_tokens:\n",
        "    for _ in range(5):\n",
        "        i = np.random.randint(0, len(ref_tokens))\n",
        "        o_t = ref_tokens[i]\n",
        "        sample = np.concatenate([[c_t], [o_t]])\n",
        "        positives_val.append(sample)\n",
        "        val_labels_pair.append(1)\n",
        "val_x_pair = np.concatenate([negatives_val, positives_val])\n",
        "val_y_pair = np_utils.to_categorical(val_labels_pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxFSrpS0HYOi"
      },
      "source": [
        "len(train_x_pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtkwJaqG9mxw"
      },
      "source": [
        "class PairBiLSTM(object):\n",
        "    def __init__(self, vocabulary_size, max_sentence_length, labels,\n",
        "                 embedding_weights, embedding_size=100):\n",
        "        self.model = None\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.max_sentence_length = max_sentence_length\n",
        "        self.embedding_weights = embedding_weights\n",
        "        self.labels = labels\n",
        "        self.n_labels = 2\n",
        "    \n",
        "    def add_input_layer(self):\n",
        "        return Input(shape=(2*self.max_sentence_length, ))\n",
        "        \n",
        "    def add_embedding_layer(self, layers):\n",
        "        layers = Embedding(\n",
        "            input_dim=self.vocabulary_size,\n",
        "            output_dim=self.embedding_size,\n",
        "            weights = [self.embedding_weights],\n",
        "            input_length = self.max_sentence_length)(layers)\n",
        "        return layers\n",
        "    \n",
        "    def add_recurrent_layer(self, layers):\n",
        "        layers = Bidirectional(\n",
        "            LSTM(units=256, return_sequences=True,\n",
        "                 recurrent_dropout=0.3))(layers)\n",
        "        return layers\n",
        "    \n",
        "    def add_output_layer(self, layers):\n",
        "        layers = Dense(self.n_labels, activation='softmax')(layers)\n",
        "        return layers\n",
        "    \n",
        "    def build(self):\n",
        "        # split here\n",
        "        inputs = self.add_input_layer()\n",
        "        split = Lambda(lambda x: tf.split(x,num_or_size_splits=2, axis=1))(inputs)\n",
        "        inputs0 = split[1]\n",
        "        inputs1 = split[0]\n",
        "        layers = self.add_embedding_layer(inputs0)\n",
        "        layers = Dropout(0.5)(layers)\n",
        "        layers = self.add_recurrent_layer(layers)\n",
        "        layers = Dropout(0.5)(layers)\n",
        "        layers = Dense(32, activation='relu')(layers)\n",
        "        layers = Flatten()(layers)\n",
        "        layers = Dense(32, activation='relu')(layers)\n",
        "        layers = Dropout(0.5)(layers)\n",
        "        layers0 = Dense(16, activation='relu')(layers)\n",
        "\n",
        "        #inputs1 = Lambda(lambda x: tf.split(x,num_or_size_splits=2))(inputs)\n",
        "        layers = self.add_embedding_layer(inputs1)\n",
        "        layers = Dropout(0.5)(layers)\n",
        "        layers = self.add_recurrent_layer(layers)\n",
        "        layers = Dropout(0.5)(layers)\n",
        "        layers = Dense(32, activation='relu')(layers)\n",
        "        layers = Flatten()(layers)\n",
        "        layers = Dense(32, activation='relu')(layers)\n",
        "        layers = Dropout(0.5)(layers)\n",
        "        layers1 = Dense(16, activation='relu')(layers)\n",
        "\n",
        "        layers = Concatenate()([layers0, layers1])\n",
        "        outputs = self.add_output_layer(layers)        \n",
        "        \n",
        "        self.model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "    \n",
        "    def fit(self, X_train, y_train, epochs, batch_size=128, validation_split=0.2):\n",
        "        if self.model is None:\n",
        "            self.build()\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=5,verbose = 2)\n",
        "        return self.model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "                              validation_split=validation_split, callbacks=[early_stopping], shuffle=True)\n",
        "    \n",
        "    def predict(self, X_test):\n",
        "        return np.argmax(self.model.predict(X_test), axis=-1)\n",
        "    \n",
        "    def evaluate(self, X_test, y_test, cm=False):\n",
        "        predictions = np.argmax(self.model.predict(X_test), axis=-1).flatten()\n",
        "        true_labels = np.argmax(y_test, axis=-1).flatten()\n",
        "        print(classification_report(true_labels, predictions))\n",
        "        if cm:\n",
        "            seaborn.heatmap(\n",
        "                metrics.confusion_matrix(true_labels, predictions, labels=range(6)))\n",
        "\n",
        "\n",
        "senti_label = np.array([0,1])\n",
        "model = PairBiLSTM(\n",
        "    vocabulary_size=len(index_dict) + 1, max_sentence_length=maxlen, \n",
        "    embedding_weights = embedding_weights, labels=senti_label, embedding_size=300)\n",
        "model.build()\n",
        "model.model.summary()\n",
        "\n",
        "\n",
        "model.fit(X_train=np.array([np.concatenate([x[0], x[1]]) for x in train_x_pair]), y_train=train_y_pair, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAKjEW57B6t-"
      },
      "source": [
        "model.fit(X_train=np.array([np.concatenate([x[0], x[1]]) for x in train_x_pair]), y_train=train_y_pair, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vpOEUXVBcDl"
      },
      "source": [
        "model.evaluate(np.array([np.concatenate([x[0], x[1]]) for x in val_x_pair]), val_y_pair)\n",
        "import sklearn\n",
        "sklearn.metrics.roc_auc_score(y_true=val_y_pair, y_score=model.model.predict(np.array([np.concatenate([x[0], x[1]]) for x in val_x_pair])), multi_class='ovo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2FCl4y5KgSw"
      },
      "source": [
        "matthews_corrcoef(model.predict(np.array([np.concatenate([x[0], x[1]]) for x in val_x_pair])), val_labels_pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4dpOnJpBvdi"
      },
      "source": [
        "y_score = model.model.predict(np.array([np.concatenate([x[0], x[1]]) for x in val_x_pair]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGSz7GLBDg74"
      },
      "source": [
        "sklearn.metrics.roc_auc_score(y_true=[val_y_pair[5*i] for i in range(400)], y_score=[y_score[5*i] for i in range(400)], multi_class='ovo')\n",
        "#sklearn.metrics.roc_auc_score(y_true=[val_y_pair[5*i] for i in range(400)], y_score=[np.mean(y_score[5*i:5*(i+1)], axis=0) for i in range(400)], multi_class='ovo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NSPIu-WEs-4"
      },
      "source": [
        "correct = np.argmax([val_y_pair[5*i] for i in range(400)], axis=-1) - np.argmax([np.mean(y_score[5*i:5*(i+1)], axis=0) for i in range(400)], axis=-1)\n",
        "np.count_nonzero(correct) / 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml1gAyfHUqlz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC2wAhun1Svr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTJhd_LYUqed"
      },
      "source": [
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from flair.data import Sentence\n",
        "embedding = TransformerDocumentEmbeddings('roberta-base')\n",
        "#embedding = TransformerDocumentEmbeddings('xlnet-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hej9lYZijpKk"
      },
      "source": [
        "def get_emb(sen):\n",
        "  sentence = Sentence(sen)\n",
        "  embedding.embed(sentence)\n",
        "  emb = sentence.get_embedding().cpu().detach().numpy()\n",
        "  sentence = None\n",
        "  return emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rURzc2IV4cZ"
      },
      "source": [
        "corona_emb = [get_emb(sen) for sen in corona_texts]\n",
        "other_emb = [get_emb(sen) for sen in tqdm(other_texts)]\n",
        "#non_emb = [get_emb(sen) for sen in non_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF8tyKc9kJpc"
      },
      "source": [
        "corona_bert = np.array(corona_emb).reshape((len(corona_texts), 768))\n",
        "#non_bert = np.array(non_emb).reshape((len(non_texts), 768))\n",
        "other_bert = np.array(other_emb).reshape((len(other_texts), 768))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYbleTonVNwf"
      },
      "source": [
        "other_bert.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdOdu7IsAT16"
      },
      "source": [
        "data = np.concatenate([corona_bert, other_bert])\n",
        "ys = np.concatenate([np.zeros(len(corona_bert)), np.ones(len(other_bert)-700), np.ones(700)*2])\n",
        "train_bert, test_bert, train_ys, test_ys, train_texts, test_texts = train_test_split(data, ys, np.concatenate([corona_texts, other_texts]), test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhg_denQmlot"
      },
      "source": [
        "# Best bert:\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "clf = MLPClassifier(hidden_layer_sizes=(200,200), learning_rate_init=0.0005, random_state=4, max_iter=100).fit(train_bert, train_ys)\n",
        "clf.score(test_bert, test_ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXAZWdDau2HU"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_corrcoef(clf.predict(test_bert), test_ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYPDZH3n7ZF0"
      },
      "source": [
        "test_emb = [get_emb(sen) for sen in tqdm(test_text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq1DJDjvKUlA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF7pUvNjGmOG"
      },
      "source": [
        "test_preds = clf.predict(test_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx58lMFxKyFR"
      },
      "source": [
        "final  =[]\n",
        "for x in test_preds:\n",
        "    if x == 0:\n",
        "      final.append(1)\n",
        "    elif x==2:\n",
        "      final.append(2)\n",
        "    elif x==1:\n",
        "      final.append(3)\n",
        "len(final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySZxJ6pOHhyk"
      },
      "source": [
        "test_data[0]['id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h76eV30HZRB"
      },
      "source": [
        "np.unique(test_preds, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zdNqJkoN2T4"
      },
      "source": [
        "for i in range(20):\n",
        "  print(test_preds[i], test_data[i]['full_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfGQA2-bH1ma"
      },
      "source": [
        "test_preds=final\n",
        "with open(root+\"roberta-3.txt\", \"w\") as text_file:\n",
        "  for i in range(len(test_data)):\n",
        "    text_file.write(test_data[i]['id_str']+','+str(int(test_preds[i]))+'\\n')\n",
        "text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCS2jwgu7ZCC"
      },
      "source": [
        "train_both_emb = np.concatenate([train_lstm_emb, train_bert], axis=-1)\n",
        "val_both_emb = np.concatenate([val_lstm_emb, test_bert], axis=-1)\n",
        "temp =MLPClassifier(hidden_layer_sizes=(300,), alpha=0.0005, random_state=0, max_iter=100).fit(train_both_emb, train_labels)\n",
        "temp.score(val_both_emb, val_labels), matthews_corrcoef(temp.predict(val_both_emb), val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwvax7OA_hyK"
      },
      "source": [
        "test_data = json.load(open(root+'test_tweets.json'))\n",
        "test_text = np.array([test_data[i]['full_text'] for i in range(len(test_data))])\n",
        "tokenize = twokenize.tokenizeRawTweetText\n",
        "test_token = [preprocess(sen, tokenize) for sen in test_text]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ORtAEbByU0"
      },
      "source": [
        "len(test_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyklKqtDAUUQ"
      },
      "source": [
        "test_x = []\n",
        "for i, words in enumerate(test_token):    \n",
        "    sen = []\n",
        "    for i, w in enumerate(words):\n",
        "        if not w.isalpha():\n",
        "            continue # Skip punctuations\n",
        "        try:\n",
        "            sen.append(index_dict[w])\n",
        "        except:\n",
        "            continue\n",
        "    test_x.append(sen)\n",
        "test_x = sequence.pad_sequences(np.array(test_x), maxlen = 55)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cer9eGfrBNrw"
      },
      "source": [
        "test_pred = np.argmax(good_model.model.predict(test_x), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP1YahSMBc_O"
      },
      "source": [
        "for i in range(10):\n",
        "  print(test_pred[i], test_text[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNWyk2jMBkUX"
      },
      "source": [
        "np.unique(test_pred, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnrmBDoW7Y-V"
      },
      "source": [
        "test_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJt0ILHo7Y31"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ucjQFB7lDch"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(clf.predict_proba(test_bert)[inds].flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzrWassllLwT"
      },
      "source": [
        "plt.hist(clf.predict_proba(test_bert)[inds].flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TdBK_lyYOC3"
      },
      "source": [
        "inds = np.where(clf.predict(test_bert) - test_ys != 0)\n",
        "probs = clf.predict_proba(test_bert)\n",
        "for i in inds[0]:\n",
        "  print(test_ys[i],  probs[i], test_texts[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko4Sjog7YN__"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_ZGIohRUu3"
      },
      "source": [
        "corona_texts[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No3I6pSUWPRO"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tA47rP7XQk6"
      },
      "source": [
        "ref = corona_texts[:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqKoHlXgXQiX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zbRG7lGXQf3"
      },
      "source": [
        "twokenize.tokenizeRawTweetText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGT4eRrBXQdi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giTTka1SWPGc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTGI11scVNr_"
      },
      "source": [
        "# Kmeans cluster\n",
        "from sklearn.cluster import KMeans\n",
        "#kmeans = KMeans(n_clusters=10, random_state=0).fit(corona_bert)\n",
        "kmeans = KMeans(n_clusters=10, random_state=0).fit(train_bert)\n",
        "#np.count_nonzero(kmeans.predict(train_bert) - train_ys) / len(train_ys)\n",
        "\n",
        "# Check cluster distribution\n",
        "corona_clusters = kmeans.predict(corona_bert)\n",
        "np.unique(corona_clusters, return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0QlbupJmlu7"
      },
      "source": [
        "train_b, test_b, train_kmeans, test_kmeans = train_test_split(corona_bert, corona_clusters, test_size=0.2, random_state=0)\n",
        "clf = MLPClassifier(hidden_layer_sizes=(400,), learning_rate_init=0.001, random_state=0, max_iter=300).fit(train_b, train_kmeans)\n",
        "clf.score(test_b, test_kmeans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDZ12NfvOdRr"
      },
      "source": [
        "_, corona_ref = train_test_split(corona_bert, test_size=0.05, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4xr5lzHUe96"
      },
      "source": [
        "#train_pairs, test_pairs, train_ys, test_ys = train_test_split(, pair_ys, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Php-7uIKSw8b"
      },
      "source": [
        "np.random.seed(0)\n",
        "pair_Xs = []\n",
        "pair_ys = []\n",
        "ref_size = {0:5, 1:5}\n",
        "for i, x in enumerate(train_bert):\n",
        "    for _ in range(ref_size[train_ys[i]]):\n",
        "        pair_Xs.append(np.concatenate([x, corona_ref[np.random.randint(0, len(corona_ref))]]))\n",
        "        pair_ys.append(train_ys[i])\n",
        "pair_Xs = np.array(pair_Xs)\n",
        "pair_ys = np.array(pair_ys)\n",
        "\n",
        "pair_Xs_test = []\n",
        "pair_ys_test = []\n",
        "ref_size = {0:5, 1:5}\n",
        "for i, x in enumerate(test_bert):\n",
        "    for _ in range(ref_size[test_ys[i]]):\n",
        "        pair_Xs_test.append(np.concatenate([x, corona_ref[np.random.randint(0, len(corona_ref))]]))\n",
        "        pair_ys_test.append(test_ys[i])\n",
        "pair_Xs_test = np.array(pair_Xs_test)\n",
        "pair_ys_test = np.array(pair_ys_test)\n",
        "print(pair_Xs.shape, pair_Xs_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxfbDR9jUe66"
      },
      "source": [
        "clf = MLPClassifier(hidden_layer_sizes=(400,400), learning_rate_init=0.0005, random_state=0, max_iter=100).fit(pair_Xs, pair_ys)\n",
        "clf.score(pair_Xs_test, pair_ys_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRPzbgmtaArz"
      },
      "source": [
        "np.bincount(preds[0:5].astype(int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZR2oHVtYe_8"
      },
      "source": [
        "preds = clf.predict(test_pairs)\n",
        "preds_ori = [np.argmax(np.bincount(preds[5*i:5*(i+1)].astype(int))) for i in range(len(preds) // 5)]\n",
        "test_ys_ori = [test_ys[5*i] for i in range(len(preds) // 5)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0S8Jc7hU3F0"
      },
      "source": [
        "matthews_corrcoef(preds_ori, test_ys_ori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NrZEB1jNuLB"
      },
      "source": [
        "np.sum((corona_bert[1] - corona_bert[0])**2), np.sum((corona_bert[1] - corona_bert[12])**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InD76bmqbtmb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCv4UxCkmlr2"
      },
      "source": [
        "# Check clusters\n",
        "for i in range(10):\n",
        "  print(corona_texts[corona_clusters==i][:3])\n",
        "  print('---')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za3SvpSQyQa6"
      },
      "source": [
        "inds = np.where(clf.predict(test_bert) - test_ys != 0)\n",
        "for i in inds[0]:\n",
        "  print(test_ys[i], test_texts[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Aa78WnrUqXm"
      },
      "source": [
        "np.unique(clf.predict(corona_bert), return_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK7bt-uXAvP5"
      },
      "source": [
        "corona_\n",
        "agree = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLMwn8gxAvKH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG_3A9QDBvwL"
      },
      "source": [
        "from transformers import AutoModel, BertTokenizerFast\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwYAvcAIBvuG"
      },
      "source": [
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDEafOtvkZBe"
      },
      "source": [
        "tokens_bert_corona = tokenizer.batch_encode_plus(\n",
        "    corona_texts.tolist(),\n",
        "    max_length = 60,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "tokens_bert_non = tokenizer.batch_encode_plus(\n",
        "    non_texts.tolist(),\n",
        "    max_length = 60,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIyf-ynZkZEA"
      },
      "source": [
        "tokens_bert_non.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXMUNqu5oOZ8"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKvUAbmZkZIf"
      },
      "source": [
        "sent_id = torch.tensor(tokens_bert_non['input_ids'][0]).unsqueeze(0)\n",
        "mask = torch.tensor(tokens_bert_non['attention_mask'][0]).unsqueeze(0)\n",
        "r = bert(sent_id, mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nQnOz43o0eR"
      },
      "source": [
        "r[0].size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3lsNg8ykZNW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxzlRP_KkZLy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZncvxuinkZGp"
      },
      "source": [
        "!git clone -b master https://github.com/charles9n/bert-sklearn\n",
        "!cd bert-sklearn; pip install .\n",
        "import os\n",
        "os.chdir(\"bert-sklearn\")\n",
        "print(os.listdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbHCMkEZrM2f"
      },
      "source": [
        "from bert_sklearn import BertClassifier\n",
        "from bert_sklearn import BertRegressor\n",
        "from bert_sklearn import BertTokenClassifier\n",
        "from bert_sklearn import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNZ4paWzBvr2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wijjtw2cBvpl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtoV3PlfBvnR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS98gHZuADgv"
      },
      "source": [
        "jieba.lcut(\"啊啊\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co_7MCJx9CG0"
      },
      "source": [
        "test = [\"2333\",\"哈哈哈\",\"哈哈哈哈\", \"哈哈哈哈哈\",\"哈哈哈哈哈哈\", \"哈哈哈哈哈哈哈哈哈哈\", \"哈哈哈哈哈哈哈哈哈哈哈哈哈哈哈\"]\n",
        "max = 0\n",
        "x = []\n",
        "for s in test:    \n",
        "    sen = []\n",
        "\n",
        "    words = jieba.lcut(s)\n",
        "    print(words)\n",
        "    if len(words) > max:\n",
        "        max = len(words)\n",
        "\n",
        "    for w in words:\n",
        "        try:\n",
        "            sen.append(index_dict[w])\n",
        "        except:\n",
        "            sen.append(0)\n",
        "    x.append(sen)\n",
        "x = sequence.pad_sequences(np.array(x), maxlen = 25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_-vVMQtABRa"
      },
      "source": [
        "index_dict['ohhh']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozd-TsyWEU3G"
      },
      "source": [
        "model.model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctm_XByCd2zS"
      },
      "source": [
        "model.model.save(\"review_emotion_classifier.h5\")\n",
        "from google.colab import files\n",
        "files.download('review_emotion_classifier.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJdlzxEAoDEG"
      },
      "source": [
        "with open('index_dict.json', 'w') as f:\n",
        "  json.dump(index_dict, f)\n",
        "files.download('index_dict.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n3UaOp-pTBb"
      },
      "source": [
        "emo_model = load_model(\"review_emotion_classifier.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myccSDhj-UeE"
      },
      "source": [
        "test = [\"我笑死了\",\"死的很惨\",\"你妈死了\", \"玩游戏不如跳舞\",\"我擦了，肾没了！！！\", \"这个故事很悲催\", \"这故事没有特定主角\"]\n",
        "max = 0\n",
        "x = []\n",
        "for s in test:    \n",
        "    sen = []\n",
        "\n",
        "    words = jieba.lcut(s)\n",
        "    if len(words) > max:\n",
        "        max = len(words)\n",
        "\n",
        "    for w in words:\n",
        "        try:\n",
        "            sen.append(index_dict[w])\n",
        "        except:\n",
        "            sen.append(0)\n",
        "    x.append(sen)\n",
        "x = sequence.pad_sequences(np.array(x), maxlen = 139)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiAKlEPC-iuP"
      },
      "source": [
        "emo_model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4QyTgoqkq0G"
      },
      "source": [
        "emo_model.save(\"emotion_classifier.h5\")\n",
        "files.download(\"emotion_classifier.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}